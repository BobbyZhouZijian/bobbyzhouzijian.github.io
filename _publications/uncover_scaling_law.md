---
title: "Uncover Scaling Laws for Large Language Models via Inverse Problems"
collection: publications
category: manuscripts
permalink: /publication/uncover_scaling_law
excerpt: '<img src="/images/uncover_scaling_law.png" alt="Uncover Scaling Law Paper Demo" style="width: 100%; height: auto; margin: 1px auto; display: block; border-radius: 8px;">'
date: 2025-06-20
venue: 'Under Review at ARR'
slidesurl: # 'http://academicpages.github.io/files/slides2.pdf'
paperurl: 'https://openreview.net/forum?id=dXdfXhp6rk#discussion'
bibtexurl: 'https://bobbyzhouzijian.github.io/files/uncover_scaling_law.bib'
citation: # 'Your Name, You. (2010). &quot;Paper Title Number 2.&quot; <i>Journal 1</i>. 1(2).'
---

Large Language Models (LLMs) are large-scale pretrained models that have achieved remarkable success across diverse domains. These successes have been driven by unprecedented complexity and scale in both data and computations. However, due to the high costs of training such models, brute-force trial-and-error approaches to improve LLMs are not feasible. Inspired by the success of inverse problems in uncovering fundamental scientific laws, this position paper advocates that inverse problems can also be used to efficiently uncover scaling laws that guide the building of LLMs to achieve a desirable performance with significantly better cost-effectiveness.